{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "__draft___gated_pixel_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7HDdyqIfbsXFsdTlRmZfC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugenHotaj/nn-hallucinations/blob/master/__draft___gated_pixel_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij3tASXjtl0_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7de26c95-2bc8-497a-f267-257702e81e1e"
      },
      "source": [
        "!git clone https://www.github.com/EugenHotaj/nn-hallucinations nn_hallucinations"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nn_hallucinations' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhbHjHAGuIQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from nn_hallucinations import colab_utils\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import distributions\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKzRg91VuUTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "TRANSFORM = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    lambda x: distributions.Bernoulli(probs=x).sample()])\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=TRANSFORM),\n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True)\n",
        "test_loader = data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, download=True,transform=TRANSFORM),\n",
        "    batch_size=BATCH_SIZE*2)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLCatRxBJIBJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO(eugenhotaj): We should rethink the Masked layers. It might make sense to \n",
        "# just have a generic MaskedConv2d layer which takes in whatever kind of mask \n",
        "# in the __init__. We can then have different functions which generate masks \n",
        "# from specific papers.\n",
        "\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "\n",
        "  EXCLUDE_SELF = 0\n",
        "  INCLUDE_SELF = 1\n",
        "\n",
        "  def __init__(self, mask_type, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "    i, o, h, w = self.weight.shape\n",
        "    # TODO(eugenhotaj): Will this masking logic work correctly for even kernel \n",
        "    # sizes? If not, maybe we should raise an exception instead.\n",
        "    mask = torch.zeros((i, o, h, w))\n",
        "    mask.data[:, :, :h//2, :] = 1\n",
        "    mask.data[:, :, h//2, :w//2 + mask_type] = 1\n",
        "    self.register_buffer('mask', mask)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.weight.data *= self.mask\n",
        "    return super().forward(x)\n",
        "\n",
        "class MaskedVerticalConv2d(nn.Conv2d):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    i, o, h, w = self.weight.shape\n",
        "    mask = torch.zeros(i, o, h, w)\n",
        "    mask[:, :, :h//2 + 1, :] = 1\n",
        "    self.register_buffer('mask', mask)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.weight.data *= self.mask\n",
        "    return super().forward(x)\n",
        "\n",
        "class MaskedHorizontalConv2d(nn.Conv2d):\n",
        "\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    i, o, h, w = self.weight.shape\n",
        "    mask = torch.zeros(i, o, h, w)\n",
        "    mask[:, :, :, :w//2 + 1] = 1\n",
        "    self.register_buffer('mask', mask)\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.weight.data *= self.mask\n",
        "    return super().forward(x)\n",
        "\n",
        "\n",
        "class GatedActivation(nn.Module):\n",
        "  \n",
        "  def __init__(self, channels):\n",
        "    super().__init__()\n",
        "    self._channels = channels\n",
        "\n",
        "  def forward(self, x):\n",
        "    tanh, sigmoid = torch.split(x, self._channels, dim=1) # Assuming NHCW\n",
        "    return torch.tanh(tanh) * torch.sigmoid(sigmoid)\n",
        "\n",
        "\n",
        "class GatedPixelCNNLayer(nn.Module):\n",
        "\n",
        "  def __init__(self, channels, kernel_size=3):\n",
        "    super().__init__()\n",
        "    self._channels = channels\n",
        "    self._kernel_size = kernel_size\n",
        "    self._padding = (kernel_size - 1) // 2 # stride = 1\n",
        "\n",
        "    def conv_1x1(in_channels, out_channels):\n",
        "      return nn.Conv2d(in_channels=in_channels, \n",
        "                       out_channels=out_channels,\n",
        "                       kernel_size=1)\n",
        "\n",
        "    # Convolutions for the vertical stack. \n",
        "    self._vstack_1x1 = conv_1x1(self._channels, 2 * self._channels)\n",
        "    self._vstack_masked = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=self._channels, out_channels=self._channels,\n",
        "            kernel_size=(1, self._kernel_size), padding=(0, self._padding)),\n",
        "        MaskedVerticalConv2d(\n",
        "            in_channels=self._channels, out_channels=2 * self._channels, \n",
        "            kernel_size=(self._kernel_size, 1), padding=(self._padding, 0)))\n",
        "\n",
        "    # Convolutions for the horizontal stack.\n",
        "    self._hstack_link = conv_1x1(2 * self._channels, 2 * self._channels)\n",
        "    self._hstack_masked = MaskedHorizontalConv2d(\n",
        "        in_channels=self._channels, out_channels=2*self._channels, \n",
        "        kernel_size=(1, self._kernel_size), padding=(0, self._padding))\n",
        "    self._hstack_skip = conv_1x1(self._channels, self._channels)\n",
        "    self._hstack_residual = conv_1x1(self._channels, self._channels)\n",
        "\n",
        "    self._activation = GatedActivation(self._channels)\n",
        "    self._shift_rows = self._kernel_size//2\n",
        "    self._pad = nn.ZeroPad2d((0, 0, self._shift_rows, 0))\n",
        "\n",
        "  def _shift_down(self, tensor):\n",
        "    # TODO(eugenhotaj): Should we copy the tensor here before shifting?\n",
        "    tensor = self._pad(tensor)\n",
        "    return tensor[:, :, :-self._shift_rows, :]\n",
        "\n",
        "  def forward(self, vstack_input, hstack_input):\n",
        "    # Compute the vertical stack. \n",
        "    vstack_masked = self._vstack_masked(vstack_input)\n",
        "    vstack = self._vstack_1x1(vstack_input) + vstack_masked\n",
        "    vstack = self._activation(vstack_masked)\n",
        "\n",
        "    # Compute the horizontal stack.\n",
        "    link = self._hstack_link(self._shift_down(vstack_masked))\n",
        "    hstack = link + self._hstack_masked(hstack_input)\n",
        "    hstack = self._activation(hstack)\n",
        "    skip = self._hstack_skip(hstack)\n",
        "    hstack = self._hstack_residual(hstack) + hstack_input\n",
        "\n",
        "    return vstack, hstack, hstack"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D4npkBMTFwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GatedPixelCNN(nn.Module):\n",
        "\n",
        "  def __init__(self, \n",
        "               in_channels, \n",
        "               hidden_channels=16,\n",
        "               head_channels=32,\n",
        "               n_gated=4):\n",
        "    \"\"\"TODO\"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self._input =  MaskedConv2d(mask_type=MaskedConv2d.EXCLUDE_SELF,\n",
        "                                in_channels=in_channels,\n",
        "                                out_channels=hidden_channels,\n",
        "                                kernel_size=3,\n",
        "                                padding=1)\n",
        "    self._gated_layers = nn.ModuleList([\n",
        "        GatedPixelCNNLayer(channels=hidden_channels) for _ in range(n_gated)])\n",
        "    self._head = nn.Sequential(\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_channels, \n",
        "                  out_channels=head_channels, \n",
        "                  kernel_size=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=head_channels, \n",
        "                  out_channels=in_channels,\n",
        "                  kernel_size=1),\n",
        "        nn.Sigmoid())\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self._input(x)\n",
        "    vstack, hstack, skip = x, x, None\n",
        "    skip_connections = torch.zeros_like(x)\n",
        "    for gated_layer in self._gated_layers:\n",
        "      vstack, hstack, skip = gated_layer(vstack, hstack)\n",
        "      skip_connections += skip\n",
        "    return self._head(skip_connections)\n",
        "\n",
        "\n",
        "  def sample(self):\n",
        "    \"\"\"Samples a new image.\n",
        "    \n",
        "    Args:\n",
        "      conditioned_on: An (optional) image to condition samples on. Only \n",
        "        dimensions with values < 0 will be sampled. For example, if \n",
        "        conditioned_on[i] = -1, then output[i] will be sampled conditioned on\n",
        "        dimensions j < i. If 'None', an unconditional sample will be generated.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "      device = next(self.parameters()).device\n",
        "      conditioned_on = (torch.ones((1, 1,  28, 28)) * - 1).to(device)\n",
        "\n",
        "      for channel in range(1):\n",
        "        for row in range(28):\n",
        "          for column in range(28):\n",
        "            out = self.forward(conditioned_on)[:, channel, row, column]\n",
        "            out = distributions.Bernoulli(probs=out).sample()\n",
        "            conditioned_on[:, channel, row, column] = torch.where(\n",
        "                conditioned_on[:, channel, row, column] < 0,\n",
        "                out, \n",
        "                conditioned_on[:, channel, row, column])\n",
        "      return conditioned_on"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9DI91SY4Cg5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5a4dfb37-11c1-4586-ce06-ed8bac4ced20"
      },
      "source": [
        "IN_CHANNELS = 1\n",
        "HIDDEN_CHANNELS = 16\n",
        "HEAD_CHANNELS = 32\n",
        "N_GATED = 10\n",
        "\n",
        "N_EPOCHS = 50\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = GatedPixelCNN(IN_CHANNELS, \n",
        "                 HIDDEN_CHANNELS, \n",
        "                 HEAD_CHANNELS, \n",
        "                 N_GATED).to(colab_utils.get_device())\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "bce_loss_fn = nn.BCELoss(reduction='none')\n",
        "\n",
        "def loss_fn(x, _, preds):\n",
        "  batch_size = x.shape[0]\n",
        "  x, preds = x.view((batch_size, -1)), preds.view((batch_size, -1))\n",
        "  return bce_loss_fn(preds, x).sum(dim=1).mean()\n",
        "\n",
        "train_losses, eval_losses = colab_utils.train_andor_evaluate(\n",
        "    model, \n",
        "    loss_fn, \n",
        "    optimizer=optimizer, \n",
        "    n_epochs=N_EPOCHS, \n",
        "    train_loader=train_loader,\n",
        "    eval_loader=test_loader,\n",
        "    device=colab_utils.get_device())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1|1460]: train_loss=121.34310574544271 eval_loss=107.001422265625\n",
            "[2|1465]: train_loss=105.1091589070638 eval_loss=101.39611820068359\n",
            "[3|1462]: train_loss=98.9564100789388 eval_loss=96.34999858398437\n",
            "[4|1462]: train_loss=96.06423762207031 eval_loss=94.445388671875\n",
            "[5|1459]: train_loss=94.18185768636067 eval_loss=92.80679537353515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gjXKprXr4VA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "b5cbfbe6-40af-4f6c-a9dc-aa39d3a34831"
      },
      "source": [
        "epochs = len(train_losses)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(range(epochs), train_losses)\n",
        "plt.plot(range(epochs), eval_losses)\n",
        "plt.legend(['train', 'test'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-71fd9bb26ba2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lS_Mgj4LSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_random(model, dataset):\n",
        "  idx = np.random.choice(len(dataset))\n",
        "  img = dataset[idx][0].unsqueeze(0).to(colab_utils.get_device())\n",
        "  img_hat = model(img)\n",
        "  colab_utils.imshow(img.reshape(28, 28))\n",
        "  colab_utils.imshow(img_hat.reshape(28, 28))\n",
        "\n",
        "\n",
        "def sample(model):\n",
        "  colab_utils.imshow(model.sample().reshape(28, 28))\n",
        "\n",
        "\n",
        "def conditional_sample_random(model, dataset):\n",
        "  idx = np.random.choice(len(dataset))\n",
        "  img = dataset[idx][0].reshape(1, -1).to(colab_utils.get_device())\n",
        "  img[0, 300:600] = -1\n",
        "  colab_utils.imshow(img.reshape(28, 28))\n",
        "  plt.show()\n",
        "  print(\"Conditional Samples:\")\n",
        "  for i in range(5):\n",
        "    img_hat = model.sample(img)\n",
        "    colab_utils.imshow(img_hat.reshape(28, 28))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZZZOkGQ4gBZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "ce557252-c4ef-40d7-e869-693a7b9ca677"
      },
      "source": [
        "show_random(model, test_loader.dataset)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADoklEQVR4nO3dwU0bURhGUYyoIlXQBEoFVEkFUZqgipSRYRlZsmZiP4/fNT5nGQT25uqX8mnsw7IsT0DP8+w3AJwmTogSJ0SJE6LECVEvaz98e373X7mws99/Pw6n/t3lhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEvcx+A5zn15/Pod//+eP1Su+EvbmcECVOiBInRIkTosQJUeKEKHFClJ1zgtGtcs/XtoN2uJwQJU6IEidEiROixAlR4oQoU8oFZk4ho1PH1ntf+7mZ5bZcTogSJ0SJE6LECVHihChxQpQ4IcrOGTN7S5z9+vzjckKUOCFKnBAlTogSJ0SJE6LECVF2zglmbol2zPvhckKUOCFKnBAlTogSJ0SJE6LECVF2zgtsbYUjnw07+tpbZr4253E5IUqcECVOiBInRIkTosQJUeKEKDvnBCN74d7fDWrL7HA5IUqcECVOiBInRIkTosQJUaaUHZSnEu6HywlR4oQocUKUOCFKnBAlTogSJ0TZOTniozM7XE6IEidEiROixAlR4oQocUKUOCHKzvnNzPyKwK3ftYOex+WEKHFClDghSpwQJU6IEidEiROi7JwTlJ+ZXPv7dszbcjkhSpwQJU6IEidEiROixAlR4oQoO2fM7K3Q85wdLidEiROixAlR4oQocUKUOCHKlMKRkUfGuC6XE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I8jxnzOyPl/TMZofLCVHihChxQpQ4IUqcECVOiBInRNk5d/CoW6Gv+LsulxOixAlR4oQocUKUOCFKnBBlStnB1qQwMrXMfqSM23E5IUqcECVOiBInRIkTosQJUeKEKDvnBGtb5OjjZnbQ78PlhChxQpQ4IUqcECVOiBInRIkTouycD2ZkB33Uj/ycxeWEKHFClDghSpwQJU6IEidEiROi7Jwxe37m7f9Y+/ueBb0tlxOixAlR4oQocUKUOCFKnBAlToiyc94ZW+PjcDkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUYdlWWa/B+AElxOixAlR4oQocUKUOCFKnBD1BTlbZ6QSCxH/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAADoklEQVR4nO3dwU0bURhGUYyoIlXQBEoFVEkFUZqgipSRYRlZsmZiP4/fNT5nGQT25uqX8mnsw7IsT0DP8+w3AJwmTogSJ0SJE6LECVEvaz98e373X7mws99/Pw6n/t3lhChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEvcx+A5zn15/Pod//+eP1Su+EvbmcECVOiBInRIkTosQJUeKEKHFClJ1zgtGtcs/XtoN2uJwQJU6IEidEiROixAlR4oQoU8oFZk4ho1PH1ntf+7mZ5bZcTogSJ0SJE6LECVHihChxQpQ4IcrOGTN7S5z9+vzjckKUOCFKnBAlTogSJ0SJE6LECVF2zglmbol2zPvhckKUOCFKnBAlTogSJ0SJE6LECVF2zgtsbYUjnw07+tpbZr4253E5IUqcECVOiBInRIkTosQJUeKEKDvnBCN74d7fDWrL7HA5IUqcECVOiBInRIkTosQJUaaUHZSnEu6HywlR4oQocUKUOCFKnBAlTogSJ0TZOTniozM7XE6IEidEiROixAlR4oQocUKUOCHKzvnNzPyKwK3ftYOex+WEKHFClDghSpwQJU6IEidEiROi7JwTlJ+ZXPv7dszbcjkhSpwQJU6IEidEiROixAlR4oQoO2fM7K3Q85wdLidEiROixAlR4oQocUKUOCHKlMKRkUfGuC6XE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I8jxnzOyPl/TMZofLCVHihChxQpQ4IUqcECVOiBInRNk5d/CoW6Gv+LsulxOixAlR4oQocUKUOCFKnBBlStnB1qQwMrXMfqSM23E5IUqcECVOiBInRIkTosQJUeKEKDvnBGtb5OjjZnbQ78PlhChxQpQ4IUqcECVOiBInRIkTouycD2ZkB33Uj/ycxeWEKHFClDghSpwQJU6IEidEiROi7Jwxe37m7f9Y+/ueBb0tlxOixAlR4oQocUKUOCFKnBAlToiyc94ZW+PjcDkhSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUYdlWWa/B+AElxOixAlR4oQocUKUOCFKnBD1BTlbZ6QSCxH/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4_D09Pd63dt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "5c0d6ed2-46c1-40d8-a1b3-34bfca44d419"
      },
      "source": [
        "model_weights = model.state_dict()\n",
        "model = PixelCNN(IN_CHANNELS, HIDDEN_CHANNELS, HEAD_CHANNELS).to(\n",
        "    colab_utils.get_device())\n",
        "model.load_state_dict(model_weights)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-21cc49e0d2c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = PixelCNN(IN_CHANNELS, HIDDEN_CHANNELS, HEAD_CHANNELS).to(\n\u001b[0m\u001b[1;32m      3\u001b[0m     colab_utils.get_device())\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PixelCNN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEn_aUj_576j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "af8e6334-154a-4a90-a005-a4ffe8f588a0"
      },
      "source": [
        "sample(model)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAC00lEQVR4nO3YsQ3DMAwAwcjIahnBU2aE7BZmASGd4S/uSqph8yCgNTMPoOe4ewFgT5wQJU6IEidEiROinv8eX8fpKxcu9vm+127uckKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocULUmpm7dwA2XE6IEidEiROixAlR4oQocULUD0EoC8mGDyx+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}