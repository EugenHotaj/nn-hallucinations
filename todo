TODO:
- Implement the Discretized Logistic Mixture.
- Add a utility to visualize images with top losses.
- Create a tutorial notebook for pytorch-generative and link in README. 
- Add readthedocs for pytorch-generative.
- Create a pip package for pytorch-generative?
- Use autocasting during training.
- Add option to only keep the last n checkpoints.
- Extract 2*PI to be a constant.

Refactoring/Cleanup:
- Move GatedActivation into GatedPixelCNN since it's only used there?
- Standardize argument names and ordering across supported models.
- Rething sampling (currently we have model.sample, model.sample_fn, trainer.sample_fn)
- Fix up code TODOs.
- Abstract common parts our of *MaskedAttention classes.
- Abstract out common loss and sample functions in reproduce function?
- Add type checking.


RESEARCH QUESTIONS:
- Is it possible to train a student model with less features than a teacher model? For 
  example, the teacher model uses dataset while the student model uses PCA.
- Can we pretrain a bert style transformer, with no positional encodings, on tabular data? 
- Instead of using a full image as input, can we break it up into squares and use it as a batch?
- Distill a large autoregressive transformer into a shallow VAE. Use the shallow model to 
  speed up sampling from the transformer.
